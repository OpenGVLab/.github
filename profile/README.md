## Welcome to OpenGVLab! 👋

<!--

**Here are some ideas to get you started:**

🙋‍♀️ A short introduction - what is your organization all about?
🌈 Contribution guidelines - how can the community get involved?
👩‍💻 Useful resources - where can the community find your docs? Is there anything else the community should know?
🍿 Fun facts - what does your team eat for breakfast?
🧙 Remember, you can do mighty things with the power of [Markdown](https://docs.github.com/github/writing-on-github/getting-started-with-writing-and-formatting-on-github/basic-writing-and-formatting-syntax)
-->

![GitHub Org's stars](https://img.shields.io/github/stars/opengvlab?style=social)
[![Twitter](https://img.shields.io/twitter/url?style=social&url=https%3A%2F%2Ftwitter.com%2Fopengvlab)](https://twitter.com/opengvlab)

OpenGVLab is a community focused on generalized vision-based AI. We strive to develop models that not only excel at one vision benchmark, but can have a general understanding of vision so that little effort is needed to adapt to new vision-based tasks. We develop model architecture and release pre-trained models to the community to motivate further research in this area. We have made promising progress in terms of general vision AI, with ***109 SOTA*** rankings from our models for **Vision-Centric Multi-Modality** tasks. We hope to empower individuals and businesses by offering a higher starting point to develop vision-based AI products and lessening the burdun of building an AI model from scratch.


![Vision-Centric Multi-Modality Opensource Platform](./profile/platform.png)

# 🚀Foundation Models

* ### [InternVL](https://github.com/OpenGVLab/InternVL) 👈

  * An Open-Source Alternative to ViT-22B，plus Visual-Linguistic ability.

* ### [InternImage](https://github.com/OpenGVLab/InternImage) 👈

  * Best performing image-based universal backbone model with up to 3 billion parameters
  
  * 90.1% Top1 accuracy in ImageNet, 65.5 mAP on COCO object detection

  > Supported by
  
  <!-- * [InternGPT](https://github.com/OpenGVLab/InternGPT) - An open source demo platform where you can easily showcase your AI models. Now it supports DragGAN, ChatGPT, ImageBind, multimodal chat like GPT-4, SAM, interactive image editing, etc.
  * [GITM](https://github.com/OpenGVLab/GITM) - A novel framework integrating Large Language Models (LLMs) with text-based knowledge and memory, aiming to create Generally Capable Agents in Minecraft.
  * [VisionLLM](https://github.com/OpenGVLab/VisionLLM) - A unified perspective for vision and language tasks by treating images as a foreign language and aligning vision-centric tasks with language tasks that can be flexibly defined and managed using language instructions. -->
  <!-- * [STM-Evaluation](https://github.com/OpenGVLab/STM-Evaluation) - A unified architecture for different spatial token mixing paradigms, and make various comparisons and analyses for these "spatial token mixers". -->
  * [M3I-Pretraining](https://github.com/OpenGVLab/M3I-Pretraining) - Successfully pre-train a 1B model (InternImage-H) with M3I Pre-training and achieve new record 65.4 mAP on COCO detection test-dev, 62.5 mAP on LVIS detection minival, and 62.9 mIoU on ADE20k.  

  * [SiameseIM](https://github.com/OpenGVLab/Siamese-Image-Modeling) - New form of self-supervised learning that can learn semantic alignment and spatial sensitivity with a single dense loss.

  And more underlining algorithms, find in the [InternImage](https://github.com/OpenGVLab/InternImage) repo! 
  <!-- * [ConvMAE](https://github.com/OpenGVLab/Official-ConvMAE-Det) - Transfer learning for object detection on COCO. -->

* ### [InternVideo](https://github.com/OpenGVLab/InternVideo) 👈

  * The first video foundation model to achieve high-performance on both video and video-text tasks.
  
  * SOTA performance on 39 video datasets when released in 2022.
  
  * 91.1% Top1 accuracy in Kinetics 400, 77.2% Top1 accuracy in Something-Something V2.
  
  > Supported by

  <!-- * [LORIS](https://github.com/OpenGVLab/LORIS) - Our model generates long-term soundtracks with state-of-the-art musical quality and rhythmic correspondence
  * 🔥 [Ask-Anything](https://github.com/OpenGVLab/Ask-Anything) - A simple yet interesting tool for chatting with video -->
  * [Data](https://github.com/OpenGVLab/InternVideo/tree/main/Data) -InternVid  

  * 🔥 [Unmasked Teacher](https://github.com/OpenGVLab/unmasked_teacher) - Our scratch-built ViT-L/16 achieves SOTA performances on various video tasks.  
  
  * [VideoMAEv2](https://github.com/OpenGVLab/VideoMAEv2) - Train a video ViT model with a billion parameters, which achieves a new SOTA performance on the datasets of Kinetics and Something-Something, and many more.

  * [UniFormerV2](https://github.com/OpenGVLab/UniFormerV2) - The first model to achieve 90% top-1 accuracy on Kinetics-400.  

  * [Efficient Video Learners](https://github.com/OpenGVLab/efficient-video-recognition) - Despite with a small training computation and memory consumption, EVL models achieves high performance on Kinetics-400.

* # ✨Applications
  * [🦜 Ask-Anything](https://github.com/OpenGVLab/Ask-Anything) - **VideoChat is here!**

  * [👻GITM](https://github.com/OpenGVLab/GITM) - LLM Agent, unclok **ALL** tasks in Minecraft. 

  * [🎨InternGPT](https://github.com/OpenGVLab/InternGPT) - An open source demo platform where you can easily showcase your AI models. Now it supports DragGAN, ChatGPT, ImageBind, multimodal chat like GPT-4, SAM, interactive image editing, etc.

* # 🛠️Multimodal algorithms and tools
  * [🤖ControlLLM](https://github.com/OpenGVLab/ControlLLM) -Augment Language Models with Tools by Searching on Graphs 

  * [🦙Llama Adapter](https://github.com/OpenGVLab/LLaMA-Adapter) -Fine-tuning LLaMA to follow Instructions within 1 Hour and 1.2M Parameters
  
  * [🎵 LORIS](https://github.com/OpenGVLab/LORIS) - Generates long-term soundtracks with state-of-the-art musical quality and rhythmic correspondence

  * [👀 VisionLLM ](https://github.com/OpenGVLab/VisionLLM) - Aligning vision-centric tasks& language with the concept of “images as a foreign language” 

  * [🧩OmniQant ](https://github.com/OpenGVLab/OmniQuant) -Simple and powerful quantization technique for LLMs

  👓3D

  * [PonderV2](https://github.com/OpenGVLab/PonderV2) - 3D pre-training framework with 11 sota.

  * [HumanBench](https://github.com/OpenGVLab/HumanBench) - A Large-scale and diverse Human-centric benchmark, and many more.

  [🐑 LAMM](https://github.com/OpenGVLab/LAMM) -  Multi-Modal Large Language Models and Applications as AI Agents

* # ⚖️Evalution

   * [Multi-Modality-Arena](https://github.com/OpenGVLab/Multi-Modality-Arena) Experience Multi-Modality model performances ( MiniGPT-4, LLaMA-Adapter V2, LLaVA, BLIP-2, and many more ) by yourself. Easy to use and really fun!

* # 🏆Champion solutions 

  * [InternVideo-Perception Test challenges](https://github.com/OpenGVLab/perception_test_iccv2023)  - 1st place in the Temporal Sound Localisation task, ICCV 2023  

  * [InternVideo-Ego4D](https://github.com/OpenGVLab/ego4d-eccv2022-solutions) - 1st place in 5 Ego4D challenges, ECCV 2022

* # 💾Data & Benchmark

  * [🔥 InternVid ](https://github.com/OpenGVLab/InternVideo/tree/main/Data/InternVid)  -Video-text dataset aimed at facilitating multimodal understanding and generation，10 million video clips，from 16 popular categories.

  * [⚕️ SAM Med 2d ](https://github.com/OpenGVLab/SAM-Med2D)  -Largest medical image segmentation dataset ( 4.6M images and 19.7M masks ) to date for training models.

  *  [📽️ MVbench ](https://github.com/OpenGVLab/Ask-Anything/tree/main/video_chat2)  - Static-to-dynamic method for defining temporal-related tasks.

* ### Follow us

  * ![Twitter X logo](./profile/twitter-x-logo.svg) [Twitter](https://twitter.com/opengvlab)  

  * ![WeChat logo](./profile/icon16_wx_logo.png) [WeChat](./profile/opengv-wechat.jpeg)  

  * 🤗[Hugging Face](https://huggingface.co/OpenGVLab)
