

<!-- ![opengvlab stars](https://img.shields.io/github/stars/opengvlab?style=social) + [![Alpha-VLLM stars](https://img.shields.io/github/stars/Alpha-VLLM?style=social)](https://github.com/Alpha-VLLM) + [![uni-medical stars](https://img.shields.io/github/stars/uni-medical?style=social)](https://github.com/uni-medical)
[![Twitter](https://img.shields.io/twitter/url?style=social&url=https%3A%2F%2Ftwitter.com%2Fopengvlab)](https://twitter.com/opengvlab) -->

![Static Badge](https://img.shields.io/badge/Stars-31k-blue?style=social&logo=github)
[![Twitter](https://img.shields.io/twitter/url?style=social&url=https%3A%2F%2Ftwitter.com%2Fopengvlab)](https://twitter.com/opengvlab)

## Welcome to OpenGVLab! 👋

We are a research group from Shanghai AI Lab focused on Vision-Centric AI research. The GV in our name, OpenGVLab, means general vision, a general understanding of vision, so little effort is needed to adapt to new vision-based tasks.

We develop model architecture and release pre-trained foundation models to the community to motivate further research in this area. We have made promising progress in general vision AI, with ***109 SOTA***🚀. In 2022, our open-sourced foundation model 65.5 mAP on the COCO object detection benchmark, 91.1% Top1 accuracy in Kinetics 400, achieved landmarks for AI vision👀 tasks for image🖼️ and video📹 understanding.

Based on solid vision foundations, we have expanded to Multi-Modality models and generation AI, which aim to empower individuals and businesses by offering a higher starting point to develop vision-based AI products and lessening the burden of building an AI model from scratch.


* ### Follow us

  * ![Twitter X logo](twitter-x-logo.svg) [Twitter](https://twitter.com/opengvlab)  

  * ![WeChat logo](icon16_wx_logo.png) [WeChat](opengv-wechat.jpeg)  

  * 🤗[Hugging Face](https://huggingface.co/OpenGVLab)
