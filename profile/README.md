## Welcome to OpenGVLab! ðŸ‘‹

[Homepage](https://opengvlab.github.io/)

<!--

**Here are some ideas to get you started:**

ðŸ™‹â€â™€ï¸ A short introduction - what is your organization all about?
ðŸŒˆ Contribution guidelines - how can the community get involved?
ðŸ‘©â€ðŸ’» Useful resources - where can the community find your docs? Is there anything else the community should know?
ðŸ¿ Fun facts - what does your team eat for breakfast?
ðŸ§™ Remember, you can do mighty things with the power of [Markdown](https://docs.github.com/github/writing-on-github/getting-started-with-writing-and-formatting-on-github/basic-writing-and-formatting-syntax)
-->

OpenGVLab is a community focused on generalized vision-based AI. We strive to develop models that not only excel at one vision benchmark, but can have a general understanding of vision so that little effort is needed to adapt to new vision-based tasks. We develop model architecture and release pre-trained models to the community to motivate further research in this area. We have made promising progress in terms of general vision AI, with ***57 SOTA*** rankings from our models both for image-based and video-based tasks. We hope to empower individuals and businesses by offering a higher starting point to develop vision-based AI products and lessening the burdun of building an AI model from scratch.

![WechatIMG711](https://user-images.githubusercontent.com/123792031/233248283-956dea03-7c99-4d43-8adb-33a7a3a19f6f.jpeg)

### Our Work

* ### [InternImage](https://github.com/OpenGVLab/InternImage)
* ### [InternVideo](https://github.com/OpenGVLab/InternVideo)

* Image-based
  * [HumanBench](https://github.com/OpenGVLab/HumanBench)
  * [STM-Evaluation](https://github.com/OpenGVLab/STM-Evaluation)
  * [M3I-Pretraining](https://github.com/OpenGVLab/M3I-Pretraining)
  * [ConvMAE](https://github.com/OpenGVLab/Official-ConvMAE-Det)

* Video-based
  * [Ask-Anything](https://github.com/OpenGVLab/Ask-Anything)
  * [InternVideo-Ego4D](https://github.com/OpenGVLab/ego4d-eccv2022-solutions) - SOTA in various Ego4D challenges, ECCV 2022
  * [VideoMAEv2](https://github.com/OpenGVLab/VideoMAEv2)
  * [Unmasked Teacher](https://github.com/OpenGVLab/unmasked_teacher)
  * [UniFormerV2](https://github.com/OpenGVLab/UniFormerV2)
  * [Efficient Video Learners](https://github.com/OpenGVLab/efficient-video-recognition)

### Follow us

* [Twitter](https://twitter.com/opengvlab)
* [WeChat](./profile/opengv-wechat.jpeg)
* [Hugging Face](https://huggingface.co/OpenGVLab)
